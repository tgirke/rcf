<h2 id="introduction">Introduction</h2>
<p>This manual provides an introduction to the usage of the HPCC cluster.
All servers and compute resources of the HPCC cluster are available to researchers from all departments and colleges at UC Riverside for a minimal recharge fee <a href="/#rates">(see rates)</a>.
To request an account, please contact Thomas Girke <a href="&#109;&#097;&#105;&#108;&#116;&#111;:&#116;&#103;&#105;&#114;&#107;&#101;&#064;&#117;&#099;&#114;&#046;&#101;&#100;&#117;">&#116;&#103;&#105;&#114;&#107;&#101;&#064;&#117;&#099;&#114;&#046;&#101;&#100;&#117;</a>.
The latest hardware/facility description for grant applications is available at <a href="/#facility-description">Facility Description</a>.</p>

<h2 id="overview">Overview</h2>

<h3 id="storage">Storage</h3>
<ul>
  <li>Four enterprise class HPC storage systems</li>
  <li>Approximately 2 PB (2048 TB) of network storage</li>
  <li>GPFS (NFS and SAMBA via GPFS)</li>
  <li>Automatic snapshots and archival backups</li>
</ul>

<h3 id="network">Network</h3>
<ul>
  <li>Ethernet
    <ul>
      <li>1 Gb/s switch x 5</li>
      <li>1 Gb/s switch 10 Gig uplink</li>
      <li>10 Gb/s switch for Campus wide Science DMZ</li>
      <li>redundant, load balanced, robust mesh topology</li>
    </ul>
  </li>
  <li>Interconnect
    <ul>
      <li>56 Gb/s InfiniBand (FDR)</li>
    </ul>
  </li>
</ul>

<h3 id="head-nodes">Head Nodes</h3>
<p>All users should access the cluster via ssh through biocluster.ucr.edu, this address will automatically balance traffic to one of the available head nodes.</p>

<ul>
  <li>Penguin
    <ul>
      <li>Resources: 8 cores, 64 GB memory</li>
      <li>Primary function: submitting jobs to the queuing system (Torque/Maui)</li>
      <li>Secondary function: development; code editing and running small (under 50 % CPU and under 30 % RAM) sample jobs</li>
    </ul>
  </li>
  <li>Pigeon
    <ul>
      <li>Resources: 16 cores, 128 GB memory</li>
      <li>Primary function: submitting jobs to the queuing system (Torque/Maui)</li>
      <li>Secondary function: development; code editing and running small (under 50 % CPU and under 30 % RAM) sample jobs</li>
    </ul>
  </li>
  <li>Pelican
    <ul>
      <li>Resources: 32 cores, 64 GB memory</li>
      <li>Primary function: submitting jobs to the queuing system (Torque/Maui)</li>
      <li>Secondary function: development; code editing and running small (under 50 % CPU and under 30 % RAM) sample jobs</li>
    </ul>
  </li>
  <li>Owl
    <ul>
      <li>Resources: 16 cores, 64 GB memory</li>
      <li>Primary function: testing; running test sets of jobs</li>
      <li>Secondary function: submitting jobs to the queuing system (Torque/Maui)</li>
    </ul>
  </li>
  <li>Globus
    <ul>
      <li>Resources: 32 cores, 32 GB memory</li>
      <li>Primary function: submitting jobs to the queuing system (Slurm)</li>
      <li>Secondary function: development; code editing and running small (under 50 % CPU and under 30 % RAM) sample jobs</li>
    </ul>
  </li>
</ul>

<h3 id="worker-nodes">Worker Nodes</h3>
<ul>
  <li>Batch
    <ul>
      <li>c01-c48: each with 64 AMD cores and 512 GB memory</li>
    </ul>
  </li>
  <li>Highmem
    <ul>
      <li>h01-h06: each with 32 Intel cores and 1024 GB memory</li>
    </ul>
  </li>
  <li>GPU
    <ul>
      <li>gpu01-gpu02: each with 32 (HT) cores Intel Haswell CPUs and 2 x NVIDIA Tesla K80 GPUs (~10000 CUDA cores) and 128 GB memory</li>
    </ul>
  </li>
  <li>Intel
    <ul>
      <li>i01-i12: each with 32 Intel Broadwell cores and  512 GB memory</li>
    </ul>
  </li>
</ul>

<h2 id="getting-started">Getting Started</h2>
<p>The initial login, brings users into a Biocluster head node (i.e. pigeon, penguin, owl). From there, users can submit jobs via qsub to the compute nodes or log into owl to perform memory intensive tasks.
Since all machines are mounting a centralized file system, users will always see the same home directory on all systems. Therefore, there is no need to copy files from one machine to another.</p>

<h3 id="login-from-mac-linux-cygwin">Login from Mac, Linux, Cygwin</h3>
<p>Open the terminal and type</p>

<div class="highlighter-rouge"><pre class="highlight"><code>ssh -X username@biocluster.ucr.edu
</code></pre>
</div>

<h3 id="login-from-windows">Login from Windows</h3>
<p>Please refer to the login instructions of our <a href="#linux-basics">Linux Basics manual</a>.</p>

<h3 id="change-password">Change Password</h3>
<ol>
  <li>Log-in via SSH using the Terminal on Mac/Linux or Putty on Windows
    <ul>
      <li>Once you have logged in type the following command:
<code class="highlighter-rouge">
passwd
</code></li>
      <li>Enter the old password (the random characters that you were given as your initial password)</li>
      <li>Enter your new password</li>
    </ul>
  </li>
</ol>

<p>The password minimum requirements are:
* Total length at least 8 characters long
* Must have at least 3 of the following:
    * Lowercase character
    * Uppercase character
    * Number
    * Punctuation character</p>

<h3 id="modules">Modules</h3>
<p>All software used on Biocluster is managed through a simple module system.
You must explicitly load and unload each package as needed.
More advanced users may want to load modules within their bashrc, bash_profile, or profile files.</p>

<h4 id="available-modules">Available Modules</h4>
<p>To list all available software modules, execute the following:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>module avail
</code></pre>
</div>

<p>This should output something like:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>------------------------- /usr/local/Modules/versions --------------------------
3.2.9
--------------------- /usr/local/Modules/3.2.9/modulefiles ---------------------
BEDTools/2.15.0<span class="o">(</span>default<span class="o">)</span> modules
PeakSeq/1.1<span class="o">(</span>default<span class="o">)</span> python/3.2.2
SOAP2/2.21<span class="o">(</span>default<span class="o">)</span> samtools/0.1.18<span class="o">(</span>default<span class="o">)</span>
bowtie2/2.0.0-beta5<span class="o">(</span>default<span class="o">)</span> stajichlab
cufflinks/1.3.0<span class="o">(</span>default<span class="o">)</span> subread/1.1.3<span class="o">(</span>default<span class="o">)</span>
matrix2png/1.2.1<span class="o">(</span>default<span class="o">)</span> tophat/1.4.1<span class="o">(</span>default<span class="o">)</span>
module-info
</code></pre>
</div>

<h4 id="using-modules">Using Modules</h4>
<p>To load a module, run:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>module load &lt;software name&gt;[/&lt;version&gt;]
</code></pre>
</div>

<p>To load the default version of the tophat module, run:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>module load tophat
</code></pre>
</div>

<h4 id="show-loaded-modules">Show Loaded Modules</h4>

<p>To show what modules you have loaded at any time, you can run:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>module list
</code></pre>
</div>

<p>Depending on what modules you have loaded, it will produce something like this:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Currently Loaded Modulefiles:
  1<span class="o">)</span> vim/7.4.1952                  3<span class="o">)</span> slurm/16.05.4                 5<span class="o">)</span> R/3.3.0                       7<span class="o">)</span> less-highlight/1.0            9<span class="o">)</span> python/3.6.0
  2<span class="o">)</span> tmux/2.2                      4<span class="o">)</span> openmpi/2.0.1-slurm-16.05.4   6<span class="o">)</span> perl/5.20.2                   8<span class="o">)</span> iigb_utilities/1
</code></pre>
</div>

<h4 id="unloading-software">Unloading Software</h4>

<p>Sometimes you want to no longer have a piece of software in path. To do this you unload the module by running:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>module unload &lt;software name&gt;
</code></pre>
</div>

<h4 id="additional-features">Additional Features</h4>
<p>There are additional features and operations that can be done with the module command. Please run the following to get more information:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>module <span class="nb">help</span>
</code></pre>
</div>

<h3 id="quotas">Quotas</h3>

<h4 id="cpu">CPU</h4>
<p>Currently, the maximum number of CPU cores a user can use simultaneously on biocluster is 256 CPU cores when the load on the cluster is &lt;30% and 128 CPU cores when the load is above 30%. If a user submits jobs for more than 256/128 CPU cores then the additional requests will be queued until resources within the user’s CPU quota become available. Upon request a user’s upper CPU quota can be extended temporarily, but only if sufficient CPU resources are available. To avoid monopolisation of the cluster by a small number of users, the high load CPU quota of 128 cores is dynamically readjusted by an algorithm that considers the number of CPU hours accumulated by each user over a period of 2 weeks along with the current overall CPU usage on the cluster. If the CPU hour average over the 2 week window exceeds an allowable amount then the default CPU quota will be reduced for such a heavy user to 64 CPU cores, and if it exceeds the allowable amount by two-fold it will be reduced to 32 CPU cores. Once the average usage of a heavy user drops again below those limits, the upper CPU limit will be raised accordingly. Note: when the overall CPU load on the cluster is below 70% then the dynamically readjusted CPU quotas are not applied. At those low load times every user has the same CPU quota: 256 CPU cores at &lt;30% load and 128 CPU cores at 30-70% load.</p>

<h4 id="data-storage">Data Storage</h4>
<p>A standard user account has a storage quota of 20GB. Much more storage space, in the range of many TBs, can be made available in a user account’s bigdata directory. The amount of storage space available in bigdata depends on a user group’s annual subscription. The pricing for extending the storage space in the bigdata directory is available <a href="/home">here</a>.</p>

<h4 id="memory">Memory</h4>
<p>From the Biocluster head node users can submit jobs to the batch queue or the highmem queue. The nodes associated with the batch queue are mainly for CPU intensive tasks, while the nodes of the highmem queue are dedicated to memory intensive tasks. The batch nodes allow a 1GB RAM minimum limit on jobs and and the highmem nodes allow 16GB-512GB RAM jobs.</p>

<h2 id="whats-next">What’s Next?</h2>
<p>You should now know the following:</p>

<ol>
  <li>Basic orginization of Biocluster
    <ul>
      <li>How to login to Biocluster</li>
      <li>How to use the Module system to gain access to Biocluster software</li>
      <li>CPU, storage, and memory limitations (quotas and hardware limits)</li>
    </ul>
  </li>
</ol>

<p>Now you can start using Biocluster.
The recommended way to run your jobs (scripts, pipelines, experiments, etc…) is to submit them to the queuing system by using sbatch.
Biocluster uses the Slurm queuing system.
Please do not run ANY computationally intensive tasks on any head node that starts with the letter “P” (i.e. penguin, pigeon, parrot). If this policy is violated, your jobs will be killed to limit the negative impact on others.
The head nodes are a shared resource and should be accessible by all users. Negatively impacting performance would affect all users on the system and will not be tolerated.</p>

<p>However you may run memory intensive jobs on Owl.
Login to Owl like so:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>ssh -X owl.ucr.edu
</code></pre>
</div>

<h2 id="managing-jobs">Managing Jobs</h2>
<p>Submitting and managing jobs is at the heart of using the cluster.  A ‘job’ refers to the script, pipeline or experiment that you run on the nodes in the cluster.</p>

<h3 id="partitions">Partitions</h3>
<p>In the past we used queues under the old Torque system, we now refer to these logically grouped nodes as partitions. There are several different partitions available for cluster users to send jobs to:</p>

<ul>
  <li>batch
    <ul>
      <li>Nodes: c01-c48</li>
      <li>Cores: AMD, 256 per user</li>
      <li>RAM: 1 GB default</li>
      <li>Time (walltime): 168 hours (7 days) default</li>
    </ul>
  </li>
  <li>highmem
    <ul>
      <li>Nodes: h01-h04</li>
      <li>Cores: Intel, 32 per user</li>
      <li>RAM: 16 GB min and 1024 GB max</li>
      <li>Time (walltime): 48 hours (2 days) default</li>
    </ul>
  </li>
  <li>gpu
    <ul>
      <li>Nodes: gpu01-gpu02</li>
      <li>Cores: Intel, 16 per user</li>
      <li>RAM: 128 GB default</li>
      <li>Time (walltime): 100 hours  default</li>
    </ul>
  </li>
  <li>intel
    <ul>
      <li>Default partition</li>
      <li>Nodes: i01-i12</li>
      <li>Cores: Intel, 64 per user</li>
      <li>RAM: 1 GB default</li>
      <li>Time (walltime): 168 hours (7 days) default</li>
    </ul>
  </li>
  <li>Group Partition
    <ul>
      <li>This partition is unique to the group, if your lab has purchased nodes then you will have a priority partition with the same name as your group (ie. girkelab).
In order to submit a job to different partitions add the optional ‘-p’ parameter with the name of the partition you want to use:</li>
    </ul>
  </li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>sbatch -p batch SBATCH_SCRIPT.sh
sbatch -p highmem SBATCH_SCRIPT.sh
sbatch -p gpu SBATCH_SCRIPT.sh
sbatch -p intel SBATCH_SCRIPT.sh
sbatch -p mygroup SBATCH_SCRIPT.sh
</code></pre>
</div>

<h3 id="slurm">Slurm</h3>
<p>Slurm is now our default queuing system across all head nodes. <a href="#getting-started">SSH directly into the cluster</a> and your connection will be automatically load balanced to a head node:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>ssh -XY biocluster.ucr.edu
</code></pre>
</div>

<h4 id="submitting-jobs">Submitting Jobs</h4>
<p>There are 2 basic ways to submit jobs; non-interactive, interactive. Slurm will automatically start within the directory where you submitted the job from, so keep that in mind when you use relative file paths.
Non-interactive submission of a SBATCH script:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>sbatch SBATCH_SCRIPT.sh
</code></pre>
</div>

<p>Here is an example of an SBATCH script:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c">#!/bin/bash -l</span>

<span class="c">#SBATCH --nodes=1</span>
<span class="c">#SBATCH --ntasks=10</span>
<span class="c">#SBATCH --cpus-per-task=1</span>
<span class="c">#SBATCH --mem-per-cpu=1G</span>
<span class="c">#SBATCH --time=1-00:15:00     # 1 day and 15 minutes</span>
<span class="c">#SBATCH --output=my.stdout</span>
<span class="c">#SBATCH --mail-user=useremail@address.com</span>
<span class="c">#SBATCH --mail-type=ALL</span>
<span class="c">#SBATCH --job-name="just_a_test"</span>
<span class="c">#SBATCH -p intel # This is the default partition, you can use any of the following; intel, batch, highmem, gpu</span>


<span class="c"># Print current date</span>
date

<span class="c"># Load samtools</span>
module load samtools

<span class="c"># Change directory to where you submitted the job from, so that relative paths resolve properly</span>
<span class="nb">cd</span> <span class="nv">$SLURM_SUBMIT_DIR</span>

<span class="c"># Concatenate BAMs</span>
samtools cat -h header.sam -o out.bam in1.bam in2.bam

<span class="c"># Print name of node</span>
hostname
</code></pre>
</div>

<p>The above job will request 1 node, 10 task (1 cpu core per task), 10GB of memory (1GB per task), for 1 day and 15 minutes. All STDOUT will be redirected to a file called “my.stdout” as well as an email sent to the user when the status of the job changes.</p>

<p>Interactive submission:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>srun --pty bash -l
</code></pre>
</div>

<p>If you do not specify a partition then the intel partition is used by default.</p>

<p>Here is a more complete example:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>srun --x11 --mem<span class="o">=</span>1gb --cpus-per-task 1 --ntasks 1 --time 10:00:00 --pty bash -l
</code></pre>
</div>

<p>The above example enables X11 forwarding and requests, 1GB of memory, 1 cores, for 10 hours within an interactive session.</p>

<h4 id="monitoring-jobs">Monitoring Jobs</h4>
<p>To check on your jobs states, run the following:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>squeue -u username
</code></pre>
</div>

<p>To list all the details of a specific job, run the following:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>scontrol show job &lt;JOBID&gt;
</code></pre>
</div>

<h4 id="advanced-jobs">Advanced Jobs</h4>
<p>There is a third way of submitting jobs by using steps.
Single Step submission:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>srun &lt;<span class="nb">command</span>&gt;
</code></pre>
</div>

<p>Under a single step job your command will hang until appropriate resources are found and when the step command is finished the results will be sent back on STDOUT. This may take some time depending on the job load of the cluster.
Multi Step submission:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>salloc -N 4 bash -l
srun &lt;<span class="nb">command</span>&gt;
...
srun &lt;<span class="nb">command</span>&gt;
<span class="nb">exit</span>
</code></pre>
</div>

<p>Under a multi step job the salloc command will request resources and then your parent shell will be running on the head node. This means that all commands will be executed on the head node unless preceeded by the srun command. You will also need to exit this shell in order to terminate your job.</p>

<h4 id="gpu-jobs">GPU Jobs</h4>
<p>A single GPU job will no longer reserve an entire node. For each node there are 4 GPUs. This means that you need to request how many GPUs that you would like to use.
Non-Interactive:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>srun -p gpu --mem<span class="o">=</span>100g --time<span class="o">=</span>1:00:00 SBATCH_SCRIPT.sh
</code></pre>
</div>

<p>Interactive</p>

<div class="highlighter-rouge"><pre class="highlight"><code>srun -p gpu --gres<span class="o">=</span>gpu:4 --mem<span class="o">=</span>100g --time<span class="o">=</span>1:00:00 --pty bash -l
</code></pre>
</div>

<p>Of course you should adjust the time argument according to your job requirements.</p>

<p>Once your job starts your code must reference the environment variable “CUDA_VISIBLE_DEVICES” which will indicate which GPUs have been assigned to your job. Most CUDA enabled software, like MegaHIT, will check this environment variable and automatically limit accordingly.</p>

<p>For example, when reserving 4 GPUs for a NAMD2 job:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="nb">echo</span> <span class="nv">$CUDA_VISIBLE_DEVICES</span>
0,1,2,3
namd2 +idlepoll +devices <span class="nv">$CUDA_VISIBLE_DEVICES</span> MD1.namd
</code></pre>
</div>

<p>Each user is limited to a maximum of 4 GPUs on the gpu partition. Please be respectful of others and keep in mind that the GPU nodes are a limited shared resource.
Since the CUDA libraries will only run with GPU hardware then development and compiling of code must be done within a job session on a GPU node, examples mentioned above.</p>

<p>Here are a few more examples of jobs that utilize more complex features (ie. array, dependency, MPI etc):
<a href="http://biocluster.ucr.edu/~jhayes/slurm/examples/">Slurm Examples</a></p>

<h2 id="data-storage-1">Data Storage</h2>
<p>HPCC cluster users are able to check on their home and bigdata storage usage from the <a href="https://dashboard.bioinfo.ucr.edu">Dashboard Portal</a>.</p>

<h3 id="home">Home</h3>
<p>Home directories are where you start each session on biocluster and where your jobs start when running on the cluster.  This is usually where you place the scripts and various things you are working on.  This space is very limited.  Please remember that the home storage space quota per user account is 20 GB.</p>

<table>
  <thead>
    <tr>
      <th>Path</th>
      <th>/rhome/<username></username></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>User  Availability</td>
      <td>All Users</td>
    </tr>
    <tr>
      <td>Node  Availability</td>
      <td>All Nodes</td>
    </tr>
    <tr>
      <td>Quota Responsibility</td>
      <td>User</td>
    </tr>
  </tbody>
</table>

<h3 id="bigdata">Bigdata</h3>
<p>Bigdata is an area where large amounts of storage can be made available to users. A lab purchases bigdata space separately from access to the cluster. This space is then made available to the lab via a shared directory and individual directories for each user.</p>

<p><strong>Lab Shared Space</strong>
This directory can be accessed by the lab as a whole.</p>

<table>
  <thead>
    <tr>
      <th>Path</th>
      <th>/bigdata/<labname>/shared</labname></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>User Availability</td>
      <td>Labs that have purchased space.</td>
    </tr>
    <tr>
      <td>Node Availability</td>
      <td>All Nodes</td>
    </tr>
    <tr>
      <td>Quota Responsibility</td>
      <td>Lab</td>
    </tr>
  </tbody>
</table>

<p><strong>Individual User Space</strong>
This directory can be accessed by specific lab members.</p>

<table>
  <thead>
    <tr>
      <th>Path</th>
      <th>/bigdata/<labname>/<username></username></labname></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>User Availability</td>
      <td>Labs that have purchased space.</td>
    </tr>
    <tr>
      <td>Node Availability</td>
      <td>All Nodes</td>
    </tr>
    <tr>
      <td>Quota Responsibility</td>
      <td>Lab</td>
    </tr>
  </tbody>
</table>

<h3 id="non-persistent-space">Non-Persistent Space</h3>
<p>Frequently, there is a need to do things like, output a significant amount of intermediate data during a job, access a dataset from a faster medium than bigdata or the home directories or write out lock files. These types of things are well suited to the use of non-persistent spaces. Below are the filesystems available on biocluster.</p>

<p><strong>RAM Space</strong>
This type of space takes away from physical memory but allows extremely fast access to the files located on it. When submitting a job you will need to factor in the space your job is using in RAM as well. For example, if you have a dataset that is 1G in size and use this space, it will take at least 1G of RAM.</p>

<table>
  <thead>
    <tr>
      <th>Path</th>
      <th>/dev/shm</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>User Availability</td>
      <td>All Users</td>
    </tr>
    <tr>
      <td>Node Availability</td>
      <td>All Nodes</td>
    </tr>
    <tr>
      <td>Quota Responsibility</td>
      <td>N/A</td>
    </tr>
  </tbody>
</table>

<p><strong>Temporary Space</strong>
This is a standard space available on all Linux systems. Please be aware that it is limited to the amount of free disk space on the node you are running on.</p>

<table>
  <thead>
    <tr>
      <th>Path</th>
      <th>/tmp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>User Availability</td>
      <td>All Users</td>
    </tr>
    <tr>
      <td>Node Availability</td>
      <td>All Nodes</td>
    </tr>
    <tr>
      <td>Quota Responsibility</td>
      <td>N/A</td>
    </tr>
  </tbody>
</table>

<p><strong>SSD Backed Space</strong>
This space is much faster than the standard temporary space, but slower than using RAM based storage.</p>

<table>
  <thead>
    <tr>
      <th>Path</th>
      <th>/scratch</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>User Availability</td>
      <td>All Users</td>
    </tr>
    <tr>
      <td>Node Availability</td>
      <td>All Nodes</td>
    </tr>
    <tr>
      <td>Quota Responsibility</td>
      <td>N/A</td>
    </tr>
  </tbody>
</table>

<h2 id="usage-and-quotas">Usage and Quotas</h2>
<p>To quickly check your usage and quota limits:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>check_quota home
check_quota bigdata
</code></pre>
</div>

<p>To get the usage of your current directory, run the following command:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>du -sh .
</code></pre>
</div>

<p>To calculate the sizes of each separate sub directory, run:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>du -shc <span class="k">*</span>
</code></pre>
</div>

<p>This may take some time to complete, please be patient.</p>

<p>For more information on your home directory, please see the Orientation section in the  Linux Basics manual.</p>

<h3 id="sharing-data-with-other-users">Sharing data with other users</h3>
<p>It is useful to share data and results with other users on the cluster, and we encourage collaboration  The easiest way to share a file is to place it in a location that both users can access. Then the second user can simply copy it to a location of their choice. However, this requires that the file permissions permit the second user to read the file.
Basic file permissions on Linux and other Unix like systems are composed of three groups: owner, group, and other. Each one of these represents the permissions for different groups of people: the user who owns the file, all the group members of the group owner, and everyone else, respectively  Each group has 3 permissions: read, write, and execute, represented as r,w, and x. For example the following file is owned by the user ‘jhayes’ (with read, write, and execute), owned by the group ‘operations’ (with read and execute), and everyone else cannot access it.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">jhayes@pigeon:~$ </span>ls -l myFile
-rwxr-x---   1 jhayes bioinfo 1.6K Nov 19 12:32 myFile
</code></pre>
</div>

<p>If you wanted to share this file with someone outside the ‘operations’ group, read permissions must be added to the file for ‘other’.</p>

<h3 id="set-default-permissions">Set Default Permissions</h3>

<p>In Linux, it is possible to set the default file permission for new files. This is useful if you are collaborating on a project, or frequently share files and  you do not want to be constantly adjusting permissions  The command responsible for this is called ‘umask’. You should first check what your default permissions currently are by running ‘umask -S’.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">jhayes@pigeon:~$ </span><span class="nb">umask</span> -S
<span class="nv">u</span><span class="o">=</span>rwx,g<span class="o">=</span>rx,o<span class="o">=</span>rx
</code></pre>
</div>

<p>To set your default permissions, simply run umask with the correct options. Please note, that this does not change permissions on any existing files, only new files created after you update the default permissions. For instance, if you wanted to set your default permissions to you having full control, your group being able to read and execute your files, and no one else to have access, you would run:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">jhayes@pigeon:~$ </span><span class="nb">umask </span><span class="nv">u</span><span class="o">=</span>rwx,g<span class="o">=</span>rx,o<span class="o">=</span>
</code></pre>
</div>

<p>It is also important to note that these settings only affect your current session.
If you log out and log back in, these settings will be reset.
To make your changes permanent you need to add them to your <code class="highlighter-rouge">.bashrc</code> file, which is a hidden file in your home directory (if you do not have a <code class="highlighter-rouge">.bashrc</code> file, you will need to create an empty file called <code class="highlighter-rouge">.bashrc</code> in your home directory).
Adding umask to your <code class="highlighter-rouge">.bashrc</code> file is as simple as adding your umask command (such as <code class="highlighter-rouge">umask u=rwx,g=rx,o=r</code>) to the end of the file.
Then simply log out and back in for the changes to take affect. You can double check that the settings have taken affect by running <code class="highlighter-rouge">umask -S</code>.</p>

<h3 id="futher-reading">Futher Reading</h3>

<ul>
  <li><a href="http://manuals.bioinformatics.ucr.edu/home/linux-basics#TOC-Permissions-and-Ownership">UNIX File Permissions Tutorial</a></li>
  <li><a href="http://www.cyberciti.biz/tips/understanding-linux-unix-umask-value-usage.html">What is Umask and How To Setup Default umask Under Linux?</a></li>
</ul>

<h3 id="copying-bigdata">Copying bigdata</h3>

<p>Rsync can:</p>

<ul>
  <li>Copy (transfer) directories between different locations</li>
  <li>Perform transfers over the network via SSH</li>
  <li>Compare large data sets (<code class="highlighter-rouge">-n, --dry-run</code> option)</li>
  <li>Resume interrupted transfers</li>
</ul>

<p>Rsync Notes:
* Rsync can be used on Windows, but you must install <a href="https://cygwin.com">Cygwin</a>. Most Mac and Linux systems already have rsync install by default.
* Always put the / after both folder names, e.g: <code class="highlighter-rouge">FOLDER_A/</code> Failing to do so will result in the nesting folders every time you try to resume. If you don’t put / you will get a second folder_B inside folder_B <code class="highlighter-rouge">FOLDER_A/FOLDER_A/</code>
* Rsync only copies by default.
* Once the rsync command is done, run it again. The second run will be shorter and can be used as a double check. If there was no output from the second run then nothing changed.
* To learn more try <code class="highlighter-rouge">man rsync</code></p>

<p>If you are transfering to, or from, your laptop/workstation it is required that you run the rsync command locally from your laptop/workstation.</p>

<p>To transfer to the cluster:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>rsync -av --progress FOLDER_A/ biocluster.ucr.edu:FOLDER_A/
</code></pre>
</div>

<p>To transfer from the cluster:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>rsync -av --progress biocluster.ucr.edu:FOLDER_A/ FOLDER_A/
</code></pre>
</div>

<p>Rsync will use SSH and will ask you for your cluster password, the same way SSH or SCP does.</p>

<p>If your rsync transer was interrupted, rsync can continue where it left off. Simply run the same command again to resume.</p>

<h3 id="copying-large-folders-on-the-cluster-between-directories">Copying large folders on the cluster between Directories</h3>

<p>If you want to syncronize the contents from one directory to another, then use the following:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>rsync -av --progress PATH_A/FOLDER_A/ PATH_B/FOLDER_A/
</code></pre>
</div>

<p>Rsync does not move but only copies. Thus you would need to delete the original once you confirm that everything has been transfered.</p>

<h3 id="copying-large-folders-between-the-cluster-and-other-servers">Copying large folders between the cluster and other servers</h3>

<p>If you want to copy data from the cluster to your own server, or another remote system, use the following:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>rsync -ai FOLDER_A/ sever2.xyz.edu:FOLDER_A/
</code></pre>
</div>

<p>The sever2.xyz.edu machine must be a server that accepts Rsync connections via SSH.</p>

<h2 id="automatic-backups">Automatic Backups</h2>

<p>The cluster does create backups however it is still advantageous for users to periodically make copies of their critical data to a separate storage device.
The cluster is a production system for research computations with a very expensive high-performance storage infrastructure. It is not a data archiving system.</p>

<p>Home backups are created daily and kept for one month.
Bigdata backups are created weekly and kept for one month.</p>

<p>Home and bigdata backups are located under the following respective directories:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>/rhome/.snapshots/
/bigdata/.snapshots/
</code></pre>
</div>

<p>The individual snapshot directories have names with numerical values in epoch time format.
The higher the value the more recent the snapshot.</p>

<p>To view the exact time of when each snapshot was taken execute the following commands:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>mmlssnapshot home
mmlssnapshot bigdata
</code></pre>
</div>

<h2 id="databases">Databases</h2>

<h3 id="loading-databases">Loading Databases</h3>

<p><a href="http://www.ncbi.nlm.nih.gov/">NCBI</a>, <a href="http://en.wikipedia.org/wiki/Pfam#External_links">PFAM</a>, and <a href="http://www.uniprot.org/">Uniprot</a>, do not need to be downloaded by users. They are installed as modules on Biocluster.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>module load db-ncbi
module load db-pfam
module load db-uniprot
</code></pre>
</div>

<p>Specific database release numbers can be identified by the version label on the module:
```
module avail db-ncbi</p>

<p>—————– /usr/local/Modules/3.2.9/modulefiles —————–
db-ncbi/20140623(default)
```</p>

<h3 id="using-databases">Using Databases</h3>

<p>In order to use the loaded database users can simply provide the corresponding environment variable (NCBI_DB, UNIPROT_DB, PFAM_DB, etc…) for the proper path in their executables.</p>

<p>This is the old deprecated BLAST and it may not work in the near future, however if you require it:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>blastall -p blastp -i proteins.fasta -d $NCBI_DB/nr -o blastp.out
</code></pre>
</div>

<p>You can can also use this method if you require the old version of BLAST (old BLAST with legacy support):</p>

<div class="highlighter-rouge"><pre class="highlight"><code>BLASTBIN=`which legacy_blast.pl | xargs dirname`
legacy_blast.pl blastall -p blastp -i proteins.fasta -d $NCBI_DB/nr -o blast.out --path $BLASTBIN
</code></pre>
</div>

<p>This is the preferred/recommended method (BLAST+):</p>

<div class="highlighter-rouge"><pre class="highlight"><code>blastp -query proteins.fasta -db $NCBI_DB/nr -out proteins_blastp.txt
</code></pre>
</div>

<p>Usually, we store the most recent release and 2-3 previous releases of each database. This way time consuming projects can use the same database version throughout their lifetime without always updating to the latest releases.</p>

<h2 id="parallelization-software">Parallelization Software</h2>

<h3 id="mpi-introduction">MPI Introduction</h3>

<p>MPI stands for the Message Passing Interface. MPI is a standardized API typically used for parallel and/or distributed computing.
Biocluster has a custom compiled version of OpenMPI that allows users to run MPI jobs across multiple nodes.
These types of jobs have the ability to take advantage of hundreds of CPU cores symultaniously, thus improving compute time.</p>

<p>Many implementations of MPI exists, however we only support the following:
* <a href="http://www.open-mpi.org/">Open MPI</a>
* <a href="http://www.mpich.org/">MPICH</a></p>

<p>If you need to compile an MPI application then please email support@hpcc.ucr.edu for assistance.</p>

<h3 id="namd-example">NAMD Example</h3>

<p>To run a NAMD2 process as an OpenMPI job on Biocluster:</p>

<ol>
  <li>Log-in to the cluster</li>
  <li>
    <p>Create SBATCH script</p>

    <div class="highlighter-rouge"><pre class="highlight"><code><span class="c">#!/bin/bash -l</span>

<span class="c">#SBATCH -J c3d_cr2_md</span>
<span class="c">#SBATCH -p batch</span>
<span class="c">#SBATCH -n 32</span>
<span class="c">#SBATCH --mem=16gb</span>
<span class="c">#SBATCH --time=01:00:00</span>

<span class="c"># Load needed modules</span>
<span class="c"># You could also load frequently used modules from within your ~/.bashrc</span>
module load slurm <span class="c"># Should already be loaded</span>
module load openmpi <span class="c"># Should already be loaded</span>
module load namd

<span class="c"># Run job utilizing all requested processors</span>
<span class="c"># Please visit the namd site for usage details: http://www.ks.uiuc.edu/Research/namd/</span>
mpirun --mca btl ^tcp namd2 run.conf &amp;&gt; run_bio.log
</code></pre>
    </div>
  </li>
  <li>
    <p>Submit SBATCH script to Slurm queuing system</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>sbatch run_bio.sh
</code></pre>
    </div>
  </li>
</ol>

<h2 id="monitoring-resources-and-limits">Monitoring Resources and Limits</h2>
<p>The easiest way to find out what your group (or associated Slurm account) is with the following:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>id -gn
</code></pre>
</div>

<p>The Above command will list your primary group.</p>

<p>It is possible to iterate over all the jobs in squeue and add up all the CPU cores to see how many are available for your group.
This will give you the total number of cores used by your group in the batch partition:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="nb">echo</span> <span class="k">$(</span>squeue -A <span class="nv">$GROUP</span> -p batch -o <span class="s1">'%C'</span> -t Running | grep -P <span class="s1">'^[0-9]'</span> | tr <span class="s1">'\n'</span> <span class="s1">'+'</span> | sed <span class="s1">'s/+$//g'</span><span class="k">)</span> | bc
</code></pre>
</div>

<p>However this does not tell you when your job will start, since it depends on the duration of each job.
The best way to do this is with the “–start” flag on the squeue command:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>squeue --start -u <span class="nv">$USER</span>
</code></pre>
</div>

<p>Also, if you want to see your limits you can do that with the following:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>sacctmgr show account <span class="nv">$GROUP</span> <span class="nv">format</span><span class="o">=</span>Account,User,Partition,GrpCPUs,GrpMem,GrpNodes --ass | grep <span class="nv">$USER</span>
</code></pre>
</div>

<p>And for your overall CPU limit for the group:
<code class="highlighter-rouge">bash
sacctmgr show account $GROUP format=Account,User,Partition,GrpCPUs,GrpMem,GrpNodes --ass | head -3
</code></p>

<h2 id="communicating-with-others">Communicating with others</h2>

<p>The cluster is a shared resource, and communicating with other users can help to schedule large computations.</p>

<p><strong>Looking-Up Specific Users</strong></p>

<p>A convenient overview of all users and their lab affiliations can be retrieved with the following command:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>user_details.sh
</code></pre>
</div>

<p>You can search for specific users by running:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="nv">MATCH1</span><span class="o">=</span><span class="s1">'tgirke'</span> <span class="c"># Searches by real name, and username, and email address and PI name</span>
<span class="nv">MATCH2</span><span class="o">=</span><span class="s1">'jhayes'</span>
user_details.sh | grep -P <span class="s2">"</span><span class="nv">$MATCH1</span><span class="s2">|</span><span class="nv">$MATCH2</span><span class="s2">"</span>
</code></pre>
</div>

<p><strong>Listing Users with Active Jobs on the Cluster</strong>
To get a list of usernames:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>qstat | awk <span class="s1">'{print $3}'</span> | sort | uniq | grep <span class="s2">"^[^-N]"</span>
</code></pre>
</div>

<p>To get the list of real names:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>grep &lt;<span class="o">(</span>user_details.sh | awk <span class="s1">'{print $2,$3,$4}'</span><span class="o">)</span> -f &lt;<span class="o">(</span>qstat | awk <span class="s1">'{print $3}'</span> | sort | uniq | grep <span class="s2">"^[^-N]"</span><span class="o">)</span> | awk <span class="s1">'{print $1,$2}'</span>
</code></pre>
</div>

<p>To get the list of emails:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>grep &lt;<span class="o">(</span>user_details.sh | awk <span class="s1">'{print $4,$5}'</span><span class="o">)</span> -f &lt;<span class="o">(</span>qstat | awk <span class="s1">'{print $3}'</span> | sort | uniq | grep <span class="s2">"^[^-N]"</span><span class="o">)</span> | awk <span class="s1">'{print $2}'</span>
</code></pre>
</div>

<h2 id="sharing-files-on-the-web">Sharing Files on the Web</h2>

<p>Simply create a symbolic link or move the files into your html directory when you want to share them.
For exmaple, log into Biocluster and run the following:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c"># Make new web project directory</span>
mkdir www-project

<span class="c"># Create a default test file</span>
<span class="nb">echo</span> <span class="s1">'&lt;h1&gt;Hello!&lt;/h1&gt;'</span> &gt; ./www-project/index.html

<span class="c"># Create shortcut/link for new web project in html directory </span>
ln -s <span class="sb">`</span><span class="nb">pwd</span><span class="sb">`</span>/www-project ~/.html/
</code></pre>
</div>

<p>Now, test it out by pointing your web-browser to http://biocluster.ucr.edu/~username/www-project/
Be sure to replace <code class="highlighter-rouge">username</code> with your actual user name.</p>

<h2 id="password-protect-web-pages">Password Protect Web Pages</h2>

<p>Files in web directories can be password protected.
First create a password file and then create a new user:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>touch ~/.html/.htpasswd
htpasswd ~/.html/.htpasswd newwebuser
</code></pre>
</div>

<p>This will prompt you to enter a password for the new user ‘newwebuser’.
Create a new directory, or go to an existing directory, that you want to password protect:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>mkdir ~/.html/locked_dir
<span class="nb">cd</span> ~/.html/locked_dir
</code></pre>
</div>

<p>For the above commands you can choose any directory name you want.</p>

<p>Then place the following content within a file called <code class="highlighter-rouge">.htaccess</code>:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="nl">AuthName</span> 'Please login'
<span class="nl">AuthType</span> Basic
<span class="nl">AuthUserFile</span> /rhome/username/.html/.htpasswd
<span class="nl">require</span> user newwebuser
</code></pre>
</div>

<p>Now, test it out by pointing your web-browser to http://biocluster.ucr.edu/~username/locked_dir
Be sure to replace <code class="highlighter-rouge">username</code> with your actual user name for the above code and URL.</p>

